{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/kokchun/Deep-learning-AI21/blob/main/Lectures/Lec7-Text_preprocessing.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; for interacting with the code"]},{"cell_type":"markdown","metadata":{},"source":["---\n","# Lecture notes - Text preprocessing\n","---\n","\n","This is the lecture note for **text preprocessing**. \n","\n","<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to text preprocessing. I encourage you to read further about text preprocessing. </p>\n","\n","Read more:\n","- [Text preprocessing using NLTK - pysansar blog post](https://pythonsansar.com/how-to-do-text-preprocessing-using-python-nltk/)\n","- [NLTK tokenize - NLTK](https://www.nltk.org/api/nltk.tokenize.html)\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Lower case\n","\n","Many times we don't want to set different meanings to words because they might have capitalized letters vs lower case. Then one can make it lower case to start with. "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(jokes)=97\n","ubuntu users are apt to get this joke.\n","'knock, knock.' 'who's there?' ... very long pause ... 'java.'\n","an sql query goes into a bar, walks up to two tables and asks, 'can i join you?'\n"]}],"source":["import pyjokes\n","\n","# get some text data\n","jokes = pyjokes.get_jokes()\n","print(f\"{len(jokes)=}\")\n","\n","# put 3 jokes in one\n","raw_text = f\"{jokes[1]}\\n{jokes[10]}\\n{jokes[5]}\"\n","\n","text = raw_text.lower()\n","\n","print(text)\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## Tokenize\n","\n","Tokenizer divides strings list of into substrings:\n","\n","- sentence tokenization\n","- word tokenization\n","\n","This is useful when you want to work with words or sentences or other sequences for an application."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['ubuntu users are apt to get this joke.', \"'knock, knock.'\", \"'who's there?'\", '... very long pause ...', \"'java.'\", \"an sql query goes into a bar, walks up to two tables and asks, 'can i join you?'\"]\n"]}],"source":["from nltk.tokenize import sent_tokenize\n","\n","# cuts when it finds period '.' \n","text_sentence_tokens = sent_tokenize(text)\n","print(text_sentence_tokens)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["[',', 'knock', '.', \"'\", \"'who\", \"'s\", 'there', '?', \"'\", '...']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import word_tokenize\n","\n","text_word_tokens = word_tokenize(text)\n","text_word_tokens[10:20]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', '.'], [\"'knock\", ',', 'knock', '.', \"'\"], [\"'who\", \"'s\", 'there', '?', \"'\"], ['...', 'very', 'long', 'pause', '...'], [\"'java\", '.', \"'\"], ['an', 'sql', 'query', 'goes', 'into', 'a', 'bar', ',', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', ',', \"'can\", 'i', 'join', 'you', '?', \"'\"]]\n"]}],"source":["words_in_sentence_tokens = [word_tokenize(sentence) for sentence in sent_tokenize(text)]\n","print(words_in_sentence_tokens)"]},{"cell_type":"markdown","metadata":{},"source":["--- \n","## Remove noise\n","\n","Some noise in the data can change the meaning. \n","- digits\n","- punctuations\n","- stop words"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["string.punctuation='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~...'\n","['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', \"'knock\", 'knock', \"'who\", \"'s\", 'there', 'very', 'long', 'pause', \"'java\", 'an', 'sql', 'query', 'goes', 'into', 'a', 'bar', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', \"'can\", 'i', 'join', 'you']\n"]}],"source":["import string\n","\n","print(f\"{string.punctuation=}\")\n","\n","# to remove three dots\n","punctuations = string.punctuation + \"...\"\n","print(f\"{punctuations=}\")\n","\n","tokens_no_punctuations = [token for token in text_word_tokens if not token in punctuations]\n","print(tokens_no_punctuations)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["swedish_stopwords=['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på', 'den', 'med', 'var', 'sig', 'för', 'så', 'till', 'är', 'men', 'ett', 'om', 'hade', 'de', 'av', 'icke', 'mig', 'du', 'henne', 'då', 'sin', 'nu', 'har', 'inte', 'hans', 'honom', 'skulle', 'hennes', 'där', 'min', 'man', 'ej', 'vid', 'kunde', 'något', 'från', 'ut', 'när', 'efter', 'upp', 'vi', 'dem', 'vara', 'vad', 'över', 'än', 'dig', 'kan', 'sina', 'här', 'ha', 'mot', 'alla', 'under', 'någon', 'eller', 'allt', 'mycket', 'sedan', 'ju', 'denna', 'själv', 'detta', 'åt', 'utan', 'varit', 'hur', 'ingen', 'mitt', 'ni', 'bli', 'blev', 'oss', 'din', 'dessa', 'några', 'deras', 'blir', 'mina', 'samma', 'vilken', 'er', 'sådan', 'vår', 'blivit', 'dess', 'inom', 'mellan', 'sådant', 'varför', 'varje', 'vilka', 'ditt', 'vem', 'vilket', 'sitta', 'sådana', 'vart', 'dina', 'vars', 'vårt', 'våra', 'ert', 'era', 'vilkas']\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/kokchungiang/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","\n","# if you haven't downloaded stopwords before\n","nltk.download(\"stopwords\")\n","\n","swedish_stopwords = stopwords.words(\"swedish\")\n","\n","print(f\"{swedish_stopwords=}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["english_stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["english_stopwords = stopwords.words(\"english\")\n","print(f\"{english_stopwords=}\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['ubuntu', 'users', 'apt', 'get', 'joke', \"'knock\", 'knock', \"'who\", \"'s\", 'long', 'pause', \"'java\", 'sql', 'query', 'goes', 'bar', 'walks', 'two', 'tables', 'asks', \"'can\", 'join']\n"]}],"source":["tokens_no_stop = [token for token in tokens_no_punctuations if token not in english_stopwords]\n","print(tokens_no_stop)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## Stemming\n","\n","Convert words into their root word. For example loves, loving --> love. There are several stemmers available and different stemmers use different rules, yielding in different results. Some are more aggressive than others, and you should read about them to choose one that is suitable for your use case. "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original tokens = ['ubuntu', 'users', 'apt', 'get', 'joke', \"'knock\", 'knock', \"'who\", \"'s\", 'long', 'pause', \"'java\", 'sql', 'query', 'goes', 'bar', 'walks', 'two', 'tables', 'asks', \"'can\", 'join']\n","snowball_stemmed_tokens= ['ubuntu', 'user', 'apt', 'get', 'joke', 'knock', 'knock', 'who', \"'s\", 'long', 'paus', 'java', 'sql', 'queri', 'goe', 'bar', 'walk', 'two', 'tabl', 'ask', 'can', 'join']\n","lancaster_stemmed_tokens= ['ubuntu', 'us', 'apt', 'get', 'jok', \"'knock\", 'knock', \"'who\", \"'s\", 'long', 'paus', \"'java\", 'sql', 'query', 'goe', 'bar', 'walk', 'two', 'tabl', 'ask', \"'can\", 'join']\n"]}],"source":["from nltk import SnowballStemmer, LancasterStemmer\n","\n","snowball = SnowballStemmer(\"english\")\n","lancaster = LancasterStemmer()\n","\n","snowball_stemmed_tokens = [snowball.stem(token) for token in tokens_no_stop]\n","lancaster_stemmed_tokens = [lancaster.stem(token) for token in tokens_no_stop]\n","\n","# different results with different types of stemmers, you should read and try them out\n","print(f\"Original tokens = {tokens_no_stop}\")\n","print(f\"{snowball_stemmed_tokens= }\")\n","print(f\"{lancaster_stemmed_tokens= }\")"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","Kokchun Giang\n","\n","[LinkedIn][linkedIn_kokchun]\n","\n","[GitHub portfolio][github_portfolio]\n","\n","[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n","[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n","\n","---\n"]}],"metadata":{"interpreter":{"hash":"154148820e5960c369045d5805f21e439d6bac55dccbedcddc8bab190d3b15e8"},"kernelspec":{"display_name":"Python 3.9.9 ('Deep-learning-exploration-QsBe511t')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":4}
